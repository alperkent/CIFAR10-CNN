import numpy as np
import matplotlib.pyplot as plt
import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms

# unpickles CIFAR-10 data as instructed in README file
def unpickle(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

# creates a dataset file suitable for generators 
class Dataset(torch.utils.data.Dataset):
  def __init__(self, data, labels, transform):
        self.data = data
        self.labels = labels
        self.transform = transform

  def __len__(self):
        return len(self.data)

  def __getitem__(self, index):
        X = self.transform(self.data[index])
        y = self.labels[index] 
        return X, y

# creates the CNN
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5, 1, 2)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 12, 5, 1, 2)
        self.conv3 = nn.Conv2d(12, 24, 5, 1, 2)
        self.fc1 = nn.Linear(24 * 8 * 8, 120)
        self.fc2 = nn.Linear(120, 60)
        self.fc3 = nn.Linear(60, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = F.relu(self.conv3(x))
        x = torch.flatten(x, 1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# CUDA for PyTorch
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
torch.backends.cudnn.benchmark = True

# loads training data 
train1 = unpickle('cifar10_data/cifar10_data/data_batch_1')
train2 = unpickle('cifar10_data/cifar10_data/data_batch_2')
train3 = unpickle('cifar10_data/cifar10_data/data_batch_3')
train4 = unpickle('cifar10_data/cifar10_data/data_batch_4')
trainset = np.concatenate((train1[b'data'], train2[b'data'], train3[b'data'], train4[b'data'])).reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))
trainlabels = torch.tensor(np.concatenate((train1[b'labels'], train2[b'labels'], train3[b'labels'], train4[b'labels'])), dtype=torch.long, device=device)
del train1, train2, train3, train4

# loads validation data
train5 = unpickle('cifar10_data/cifar10_data/data_batch_5')
valset = train5[b'data'].reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))
vallabels = torch.tensor(train5[b'labels'], dtype=torch.long, device=device)
del train5

# loads test data
test = unpickle('cifar10_data/cifar10_data/test_batch')
testset = test[b'data'].reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))
testlabels = torch.tensor(test[b'labels'], dtype=torch.long, device=device)
del test

# loads class labels
label_info = unpickle('cifar10_data/cifar10_data/batches.meta')
classes = [x.decode('utf-8') for x in label_info[b'label_names']] 
del label_info

# parameters
batch_size = 4
max_epochs = 5

# transformations
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

# loaders
training_set = Dataset(trainset, trainlabels, transform)
trainloader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True)

validation_set = Dataset(valset, vallabels, transform)
valloader = torch.utils.data.DataLoader(validation_set, batch_size=batch_size)

test_set = Dataset(testset, testlabels, transform)
testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)

network = CNN()
network.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)

# # functions to show an image
# def imshow(img):
#     img = img / 2 + 0.5     # unnormalize
#     npimg = img.numpy()
#     plt.imshow(np.transpose(npimg, (1, 2, 0)))
#     plt.show()

# # get some random training images
# dataiter = iter(trainloader)
# images, labels = dataiter.next()

# # show images
# imshow(torchvision.utils.make_grid(images))
# # print labels
# print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))

# loop over epochs
losses = []
train_acc = []
val_acc = []
for epoch in range(max_epochs):
    # training
    running_loss = 0.0
    train_correct = 0
    train_total = 0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = network(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        _, predicted = torch.max(outputs.data, 1)
        train_total += labels.size(0)
        train_correct += (predicted == labels).sum().item()
        running_loss += loss.item()
    train_acc.append(100*train_correct/train_total)
    losses.append(running_loss/train_total)
        
    # validation
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in valloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = network(inputs)
            _, predicted = torch.max(outputs.data, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()
    val_acc.append(100*val_correct/val_total)

print('Training accuracy: %d %%' % train_acc[-1])
print('Validation accuracy: %d %%' % val_acc[-1])